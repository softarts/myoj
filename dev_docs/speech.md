
# yitu 语音回顾
2021-10与mingyu的交流
声学模型 - 深度神经网络(RNN，LSTM，)进行训练
decode 解码使用openFST (WFST)

## 产品架构
应用服务器-> RabbitMQ(替代Kafka)->(OSS,redis作缓存）-> ASR 算法服务器，顺滑，打标点
指标 CER,WER, SER
短语音, job走MQ, 数据在redis, 结果也在MQ; 长语音,数据和结果写到OSS
实时语音， 走GRPC(支持streamming)， 理论上用websocket也可以

**kafka 不用的原因**
一个partition已经有一条长语音的任务了，结果又来了一条短语音，会被阻塞。
当增加client/partiton的时候，会造成client重新连接（主要还是kafka语义的问题）
=>这是由于欠缺worker负载管理造成的,应该根据worker负载决定如何分发任务


client ===> webservice || GRPC <==> [ kafka ] ==> worker(ASR/SDK (multithreading, raw api)) ==> VAD
                               <== 结果
                               <==> OSS/database
其它： 计费
zabbix agent

## 分工
算法组交付: 语言模型，声学模型,报告,CER/WER
团队: mingyu C++SDK，对模型的加密,预处理,线程安全/并发处理，GRPC服务化, 将耗时任务分解，重新编排。成公共服务，
应用服务器: fengchang, spring/tomcat, 鉴权， swagger API, 音频上传(PCM,WAV, 16KHz, 8/16bit)，发送任务
其它：断句(VAD), 打标点，顺滑，性能8路/GPU，改进->100路/GPU 通过batch
语言模型对内存要求太大:几十G， 改造-> 优化动态解码的版本

训练过程:喜马拉雅，自己收集的会议录音

应用
toB: 
公共安全(监狱)，会议本(记录转写,说话人分离6个人，声纹识别）， 实时语音转写(1秒latency,95%）， 实时上屏，热词
长语音文件转写(100s??/s,98%准确率)相当于1个小时36秒出数据?
toC：
内容审核(直播),

dailogflow
intents(意图), entities(关键字，输入更多的类似词协助训练)
创建一个intent，并且添加不同的问法协助训练，以及增加followup intent
使用fulfillment来响应
还有一些应用新加坡本地做的demo, 前台助理， 人脸识别(yitu)，交互dialogflow, intent, 



## kaldi
### 名词解释
CTC 是一种让网络自动学会对齐的好方法,可以看成是一种损失函数? 前向后向算法（encode,softmax,CTC loss)
EM 期望最大化(EM)算法
GMM 概率观察函数?
HMM 隐马尔可夫链模型
HCLG WFST 网络的四个部分
MFCC 梅尔频率倒谱系数(MFCC)
N-gram, nnlm 语言模型
SRILM 语言模型训练工具？
THK token passing 维特比算法的一种实现？
Viterbi 图里的搜索最优路径算法
WFST 加权有限状态转换机


kaldi 是一个传统的语音识别工具框架，GMM-HMM模型，
RNNLM, tensorflow

### 端到端
端到端系统的输入为声学特征序列，输出对应的单词/字序列，以CTC和Seq2Seq两种结构为主。语言模型在两种系统中都起着很大的作用，而WFST则主要应用于传统的系统。本文内容将基于HMM的传统语音识别展开介绍。
依图的也用了WFST?

### 声学模型
有了具体的分类的目标（比如三音子）之后，下面就要选择具体的数学模型进行声学建模。这里可以根据语音学等研究 使用多种线性结构或非线性结构的模型或模型组合。目前最广泛使用的仍然是基于隐式马尔科夫模型的建模方法，即对每个三音子分别建立一个模型，具体可以参见楼上的回答。隐式马尔科夫模型的转移概率密度以几何分布最为常见，但语音合成中也常用高斯分布；观测概率密度函数传统上通常使用 高斯混合模型，也有人使用人工神经网络等，近年来随着深度学习的发展，使用各种深层神经网络的情况 越来越多。最近也有人使用不同方法直接利用递归神经网络进行建模，有一些工作也取得了比较好的效果。但无论使用哪种模型甚至非线性的模型 组合，背后的含义都是假设了对应于每种 类别（三音子）的语音帧在它所对应的高维空间中具有几乎确定的空间分布，可以通过对空间进行划分，并由未知语音帧的空间位置来对语音帧进行正确的分类。

### HMM-GMM 建立声学模型的几个步骤

### 高斯混合模型GMM
利用高斯混合模型进行聚类，本质上可以这么理解：
数据的分布由若干高斯分布组合而成，需要通过传入的无标记数据，求解出各个高斯模型的参数和各个模型的先验概率！不同于一般利用最大似然估计参数的情况在于由于传入的数据无标记，也就是说缺少了观测数据的类别这个隐藏信息，所以这个隐藏信息的概率分布也成了估计内容之一，从而无法通过求偏导进行梯度下降来求解，于是利用了EM。(最大期望值)

### CTC
在基于CD-DNN-HMM架构的语音识别声学模型中，训练DNN通常需要帧对齐标签。在GMM中，这个对齐操作是通过EM算法不断迭代完成的，而训练DNN时需要用GMM进行对齐则显得非常别扭。因此一种不需要事先进行帧对齐的方法呼之欲出。
对齐
使用RNN+CTC建模实现了端到端语音识别的声学模型，CTC的全称是Connectionist Temporal Classification，中文翻译大概是连接时序分类。它要达到的目标就是直接将语音和相应的文字对应起来，实现时序问题的分类。


### WFST
端到端模型带有一个弱的语言模型，但是单独训练一个语言模型解码更好，而且要把弱语言模型去掉

至此，我们知道使用声学模型可以拟合HMM状态到声学特征序列的发射概率，也知道了状态与单词之间层层对应的关系，接下来我们需要一个模型，使其输入一个状态序列，即可高效的输出对应的单词序列，并给出相应的打分以供解码其使用，这就是WFST解决的问题

加权有限状态转换机, decode 解码器
它是一个包含了声学模型（H）、上下文相关处理的FST（context-dependency transducer, C）、发音词典（L）、语言模型(G)，这四个网络，通过一定“操作”结合，形成了解码网络。
HCLG
H 声学模型， 在基于WFST的方法里，一个声学模型可以被看成一个转换机(transducer)，其将输入的语音信号转化为一个上下文相关的phone序列(HMM)。
C 上下文相关的因子序列 -> 跨词三因子模型 -> 输出上下文无关的因子序列
L 输出词序列
G 语言模型 (n-gram和FSG finite state grammar), WFSA 输入是一个词序列,然后G用来判断概率，是否符合语法

### 发音字典
用途？
发音词典则给出了每个单词对应的音素序列，如下图所示。目前，常用的开源中文词典有thchs30和CC-CEDICT，英文词典有CMU-dict。词典中的单词决定了这个词在解码时出现的概率，所以通常需要根据业务需求挖掘词典，并生成音素序列，这个功能称为g2p (Grapheme-to-Phoneme)，目前有Sequitur G2P、g2p-seq2seq等工具可以用来训练发音词典生成模型。

传统
词典 G2P(grapheme->phoneme)处理集外词,CTC中没有了这项
分帧，加窗，提取特征(MFCC,PLP??),FBank


### 语言模型
N-Gram/RNN 语言模型，查找概率最大
但若不是仅针对数字，而是所有普通词汇，可能达到十几万个词，解码过程将非常复杂，识别结果组合太多，识别结果不会理想。因此只有声学模型是完全不够的，需要引入语言模型来约束识别结果。让“今天天气很好”的概率高于“今天天汽很好”的概率，得到声学模型概率高，又符合表达的句子。
为了解决自由参数数目过多的问题，引入了马尔科夫假设：随意一个词出现的概率只与它前面出现的有限的n-1个词有关。基于上述假设的统计语言模型被称为N-gram语言模型。
SRILM 是模型训练工具
N-gram可以编译成加权有限状态转换器，可以看成一种有向图。这样解码就变成了在图中搜索的过程，比较高效。所以语音识别一般用的还是N-gram）

### 解码
WFST,beam search, 使用viterbi算法搜索


事实上，HMM的内涵绝不是上面所说的“无非是个状态网络”，如果希望深入了解，下面给出了一些阅读材料
详细介绍了用E-M算法训练HMM参数的推导过程，首先讲E-M的基本原理，然后讲解如何应用到GMM的训练，最后讲解如何应HMM训练
sogou
目前已具备了DNN、RNN、LSTM、LSTM-CTC等建模能力，同时语音输入法积累了海量的数据更加便于我们进行模型的迭代和升级，本次演示我们使用了基于主流的LSTM-CTC的模型；

n-gram
单词概率基于前面若干个单词,最大似然估计进行计算
因为内存需求太大，使用back-off进行平滑

# KV 人脸
参考 https://www.jianshu.com/p/f889029ff151
1:N 在数据库中按相似度排序=>这人是谁
1:1 查询，两张照片是否同一人 (阈值可以调整)=> 这人是不是某人

## 架构
客户端:
人脸检测，人脸对齐, 网络处理
opencv, ncnn(model)
=>在依图的系统里就是摄像头，视频流和图片流

server端
考虑单台机器的GPU卡数量P-100？， 采用集群的部署方式
数据库，kafka,website都在master上面

人脸检测，人脸对齐(肩膀) ==>数据库， kafka
特征抽取，特征比对
----
opencv(对图像本身操作), tensorflow(的model,cnn卷积网络)+sdk，ncnn,cuda(gpu卡的api操作)

其它:
web backend/frontend, 主要是UI操作，设置，查找，对比，导出之类(这个完全可以用spring/java+mongodb)
数据库，使用了mongodb,存储人脸meta和特征矢量数据，系统配置数据
加密文件系统->保存人脸原始数据
训练->KV
应该可以参考语音

### 集群的运作方式
猜测是根据容量配置集群容量，单卡100路视频，单机x4/8/16
然后每个摄像头直接分配到对应的服务器,但查询统一从中央数据库查询
并非primary/slave的模式，而是worker的方式=>是否有现成的库?=>

集群数据传递=>kafka
KV GRPC => 对底层SDK在做跨网络调用时的封装

### GRPC
client 侧,channel, stub
server侧,service
底层的实现？ epoll
http2/protobuf，二进制，可以快速过一下，以后围绕这个

### 特征对比
估计要把数据库中的特征数据加载到显卡，然后通过sdk-api进行搜索

### 人脸原始数据的存储
人脸图，场景图，视频
专有格式，二进制，写的时候就是加密数据，使用密钥，文件以mmap 文件系统的方式加载
互操作，使用python sdk(pybind11可以调用C++ api)读取


### 对SDK的封装
raw API改写为高层的SDK
目标，线程安全，提高吞吐量(batch)，控制时延
以人脸为例，有几个步骤:
- 预处理，矫正图片，把人脸放在中间，肩膀比例
- 抽取特征因子
- 对比,1:1,1:N

资源管理(这里必然有一些全局的共享数据)，选择需要的GPU，分配计算任务
还有node节点一个级别的资源管理, 所谓任务是无状态，但是之间有一个piepeline，流水线

(GPU)底层的cuda context是线程安全的。cducnn之类
copy memory->cuda->copy back
摄像头=>opencv, cuda

线程池-参照boost-threadpool 或者threadexecutor做的in-house, fixed size,做分发
async 可能来自pool, thread则是创建线程(overhead)
同步:mutex, 
concurrent_hashmap锁=>分段及细粒度的锁，读写锁之类


### TBB 
task based programming, scheduler 适合基本没有阻塞的task,如果阻塞在IO上或者等待mutex，那并不适合
因为coroutine可以switch,而这个并不可以
fork-join pattern
基本上只适合纯计算任务







消息系统
kafka
